{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 1\n",
      "stage 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import urllib2\n",
    "import bs4 #this is beautiful soup\n",
    "\n",
    "from pandas import Series\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from pylab import *\n",
    "from ipywidgets import widgets\n",
    "from bs4 import BeautifulSoup\n",
    "print \"stage 1\"\n",
    "import codecs\n",
    "import re\n",
    "import urllib\n",
    "\n",
    "from nltk import word_tokenize\n",
    "import datetime \n",
    "import time \n",
    "import collections \n",
    "from scipy import stats\n",
    "import nltk\n",
    "\n",
    "print \"stage 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "The number of rows in nybits:  31\n",
      "this is the first column:  $4,895\n",
      "this is the first column:  $2,995\n",
      "this is the first column:  $4,800\n",
      "[u'The Epic, New York, NY, 10001', u'63 West, New York, NY, 10023', u'140 East 56th Street, New York, NY, 10022'] [u'Midtown South, Manhattan', u'Upper West Side, Manhattan', u'Sutton Place, Manhattan'] [2, 2, 2] [4895, 2995, 4800]\n",
      "26 26 26 26 26 26 26 26 26\n",
      "    bathroom  bedroom                               full_address  \\\n",
      "0        1.0        1                               West 77th St   \n",
      "1        1.0        1                              West 104th St   \n",
      "2        1.0        1                              West 105th St   \n",
      "3        1.0        1                                 West 105th   \n",
      "4        1.0        1                              West 106th St   \n",
      "5        1.0        1                                  West 78th   \n",
      "6        1.0        1                              West 106th St   \n",
      "7        1.0        1                                  West 73rd   \n",
      "8        1.0        1                               West 68th St   \n",
      "9        1.0        1                         180 W. 20th Street   \n",
      "10       1.0        1                       101 West 15th Street   \n",
      "11       1.0        1                             777 6th Avenue   \n",
      "12       1.0        1                          105 W 29th Street   \n",
      "13       1.0        1                             777 6th Avenue   \n",
      "14       1.0        1                       245 West 25th Street   \n",
      "15       1.0        1                             777 6th Avenue   \n",
      "16       1.0        1                       450 West 17th Street   \n",
      "17       1.0        1                       505 West 29th Street   \n",
      "18       1.0        1                          200 W 26th Street   \n",
      "19       1.0        1                       537 West 27th Street   \n",
      "20       1.0        1                           303 Tenth Avenue   \n",
      "21       1.0        1                       525 West 28th Street   \n",
      "22       1.0        1                       255 West 14th Street   \n",
      "23       NaN        2              The Epic, New York, NY, 10001   \n",
      "24       NaN        2               63 West, New York, NY, 10023   \n",
      "25       NaN        2  140 East 56th Street, New York, NY, 10022   \n",
      "\n",
      "             hashid                                          image_url  \\\n",
      "0   167812131115252  https://s3.amazonaws.com/nakedapartments/image...   \n",
      "1   167812131116242  https://s3.amazonaws.com/nakedapartments/image...   \n",
      "2   167812131116272  https://s3.amazonaws.com/nakedapartments/image...   \n",
      "3   167812131113272  https://s3.amazonaws.com/nakedapartments/image...   \n",
      "4   167812131116212  https://s3.amazonaws.com/nakedapartments/image...   \n",
      "5   167812131112272  https://s3.amazonaws.com/nakedapartments/image...   \n",
      "6   167812131116252  https://s3.amazonaws.com/nakedapartments/image...   \n",
      "7   167812131112292  https://s3.amazonaws.com/nakedapartments/image...   \n",
      "8   167812131115272  https://s3.amazonaws.com/nakedapartments/image...   \n",
      "9   167812131093544  http://cdn-img3.streeteasy.com/nyc/image/79/21...   \n",
      "10  167812131095594  http://cdn-img2.streeteasy.com/nyc/image/30/21...   \n",
      "11  167812131089434  http://cdn-img2.streeteasy.com/nyc/image/62/21...   \n",
      "12  167812131092574  http://cdn-img0.streeteasy.com/nyc/image/12/21...   \n",
      "13  167812131089454  http://cdn-img1.streeteasy.com/nyc/image/49/21...   \n",
      "14  167812131095494  http://cdn-img2.streeteasy.com/nyc/image/26/21...   \n",
      "15  167812131089444  http://cdn-img3.streeteasy.com/nyc/image/55/21...   \n",
      "16  167812131000504  http://cdn-img0.streeteasy.com/nyc/image/44/21...   \n",
      "17  167812131121444  http://cdn-img0.streeteasy.com/nyc/image/92/21...   \n",
      "18  167812131092404  http://cdn-img0.streeteasy.com/nyc/image/72/21...   \n",
      "19  167812131121424  http://cdn-img1.streeteasy.com/nyc/image/17/21...   \n",
      "20  167812131117464  http://cdn-img0.streeteasy.com/nyc/image/92/21...   \n",
      "21  167812131121404  http://cdn-img0.streeteasy.com/nyc/image/72/21...   \n",
      "22  167812131095414  http://cdn-img3.streeteasy.com/nyc/image/11/21...   \n",
      "23  167812132133493  http://www.nybits.com/apartmentimages/1ab65309...   \n",
      "24  167812132143303  http://www.nybits.com/apartmentimages/ipst/193...   \n",
      "25  167812132150483  http://www.nybits.com/apartmentimages/069b19b5...   \n",
      "\n",
      "                                              linkurl  \\\n",
      "0   http://www.nakedapartments.com/apartment/1-Bed...   \n",
      "1   http://www.nakedapartments.com/apartment/1-Bed...   \n",
      "2   http://www.nakedapartments.com/apartment/1-Bed...   \n",
      "3   http://www.nakedapartments.com/apartment/1-Bed...   \n",
      "4   http://www.nakedapartments.com/apartment/1-Bed...   \n",
      "5   http://www.nakedapartments.com/apartment/1-Bed...   \n",
      "6   http://www.nakedapartments.com/apartment/1-Bed...   \n",
      "7   http://www.nakedapartments.com/apartment/1-Bed...   \n",
      "8   http://www.nakedapartments.com/apartment/1-Bed...   \n",
      "9   http://streeteasy.com//building/the-westminste...   \n",
      "10  http://streeteasy.com//building/101-west-15th-...   \n",
      "11  http://streeteasy.com//building/777-6th-avenue/6f   \n",
      "12  http://streeteasy.com//building/beatrice-105-w...   \n",
      "13  http://streeteasy.com//building/777-6th-avenue...   \n",
      "14      http://streeteasy.com//building/the-arthur/1b   \n",
      "15  http://streeteasy.com//building/777-6th-avenue/3g   \n",
      "16  http://streeteasy.com//building/the-caledonia/...   \n",
      "17  http://streeteasy.com//building/abington-house...   \n",
      "18  http://streeteasy.com//building/chelsea-centro...   \n",
      "19   http://streeteasy.com//building/high-line-537/3h   \n",
      "20        http://streeteasy.com//building/port-10/phd   \n",
      "21  http://streeteasy.com//building/ava-high-line/a27   \n",
      "22  http://streeteasy.com//building/255-west-14-st...   \n",
      "23  http://www.nybits.com/apartmentlistings/1ab653...   \n",
      "24  http://www.nybits.com/apartmentlistings/193252...   \n",
      "25  http://www.nybits.com/apartmentlistings/069b19...   \n",
      "\n",
      "                  neighborhood  price  source  \n",
      "0              Upper West Side   2525       2  \n",
      "1              Upper West Side   2400       2  \n",
      "2              Upper West Side   2695       2  \n",
      "3              Upper West Side   2695       2  \n",
      "4              Upper West Side   2095       2  \n",
      "5              Upper West Side   2700       2  \n",
      "6              Upper West Side   2495       2  \n",
      "7              Upper West Side   2900       2  \n",
      "8              Upper West Side   2700       2  \n",
      "9                     Chelsea    5400       4  \n",
      "10                    Chelsea    5895       4  \n",
      "11                    Chelsea    4330       4  \n",
      "12                    Chelsea    5665       4  \n",
      "13                    Chelsea    4450       4  \n",
      "14                    Chelsea    4850       4  \n",
      "15                    Chelsea    4385       4  \n",
      "16                               5025       4  \n",
      "17               West Chelsea    4405       4  \n",
      "18                    Chelsea    3950       4  \n",
      "19               West Chelsea    4200       4  \n",
      "20               West Chelsea    4595       4  \n",
      "21               West Chelsea    4010       4  \n",
      "22                    Chelsea    4095       4  \n",
      "23    Midtown South, Manhattan   4895       3  \n",
      "24  Upper West Side, Manhattan   2995       3  \n",
      "25     Sutton Place, Manhattan   4800       3  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def calhashid(beds, neighborhood, address, price, source, dt=None):\n",
    "    \n",
    "    if (not dt) or (len(dt)!=10):\n",
    "        dt = datetime.datetime.now()\n",
    "        dt = str(dt.year%100) + str(dt.month) + str(dt.day) + str(dt.hour) + str(dt.minute)\n",
    "        \n",
    "    sid = dt\n",
    "    \n",
    "    if beds:\n",
    "        sid += str(beds).zfill(1)[:1]\n",
    "    else:\n",
    "        sid += '0'\n",
    "    \n",
    "    nlength = 0\n",
    "    nparts = 0\n",
    "    naddress = 0\n",
    "\n",
    "    if neighborhood:\n",
    "        neighborhood = neighborhood.strip()\n",
    "        nlength = len(neighborhood)\n",
    "        nparts = len(neighborhood.split())\n",
    "    \n",
    "    if address:\n",
    "        address = address.strip()\n",
    "        naddress = len(address)\n",
    "        \n",
    "    temptext='000'\n",
    "    if nlength > 0: \n",
    "        temptext = str(nlength+nparts+naddress).zfill(3)[:3]\n",
    "        temp=ord(neighborhood[0])\n",
    "        sid += str(int(temptext)+temp).zfill(3)[:3]\n",
    "    else:\n",
    "        sid+=temptext\n",
    "        \n",
    "    \n",
    "    if isinstance(price,float) or (price is None):\n",
    "        sid+='00'\n",
    "    else: \n",
    "        sid += str(round(price/100.)).zfill(2)[:2]\n",
    "\n",
    "    sid+=str(source)[:1]\n",
    "    \n",
    "    return sid\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "prices = []\n",
    "addresses = []\n",
    "neighborhoods = [] \n",
    "imagelinks = []\n",
    "urllinks = []\n",
    "beds = []\n",
    "baths = []\n",
    "ft2s = []\n",
    "sourcepages = []\n",
    "hashids = []\n",
    "\n",
    "sourcedict = {'http://www.nakedapartments.com/':2, 'http://www.nybits.com/': 3,  'http://streeteasy.com/': 4 }\n",
    "\n",
    "URL = \"http://www.nakedapartments.com/renter/listings/search?nids=3&aids=3%2C11&min_rent=&max_rent=3000&button=\"\n",
    "baseURL = re.match(r'\\w+://(.+\\.\\w+)/', URL).group(0)\n",
    "filepwd=\"test2.htm\"\n",
    "html=codecs.open(filepwd)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "rows = soup.findAll(\"div\", {\"class\":\"listing-row listing-row-standard  mappable-element ab-test mobile-web-style row\", })\n",
    "\n",
    "\n",
    "for row in rows: \n",
    "    rowimage = row.find(attrs = {\"class\": \"lazy\"})[\"data-original\"]\n",
    "    imagelinks.append(rowimage)\n",
    "#     print \"imagelink:\", rowimage[\"data-original\"]\n",
    "    rowinfo = row.find(attrs = {\"class\": \"listing-details col-xs-8\"})\n",
    "    listingtitle = rowinfo.find(attrs={\"class\": \"listing-title\"})\n",
    "#     print listingtitle.text\n",
    "#     print listingtitle.text\n",
    "    tempprice = re.match(r'\\$(\\d+),(\\d{3})', listingtitle.text)\n",
    "    prices.append(int(tempprice.group(1)+tempprice.group(2)))\n",
    "    urllinks.append(listingtitle['href'])\n",
    "    listingaddress = rowinfo.find(attrs={\"class\": \"listing-address\"})\n",
    "    sizeinfo = rowinfo.find(\"span\", {\"class\": \"listing-size\"}).text\n",
    "    bedsbaths = re.match(r'(\\d)BR, (\\d)BA', sizeinfo)\n",
    "    beds.append(int(bedsbaths.group(1)))\n",
    "    baths.append(int(bedsbaths.group(2)))\n",
    "    addresses.append(listingaddress.text)\n",
    "#     print listingaddress.text\n",
    "    listingneighb = rowinfo.find(attrs = {\"class\": \"listing-neighborhood\"})\n",
    "    neighborhoods.append(listingneighb.text)\n",
    "    ft2s.append(None)\n",
    "    sourcepages.append(sourcedict[baseURL]) \n",
    "    \n",
    "    #beds, neighborhood, address, price, source,\n",
    "    hashids.append(calhashid(beds[-1], neighborhoods[-1], addresses[-1], prices[-1], sourcepages[-1]))\n",
    "    \n",
    "\n",
    "URL= \"http://streeteasy.com/for-rent/chelsea/price:1000-6000%7Cbeds=1%7Cbaths%3E=1%7Cno_fee:1\"\n",
    "baseURL = re.match(r'\\w+://(.+\\.\\w+)/', URL).group(0)\n",
    "filepwd=\"test4.htm\"\n",
    "html=codecs.open(filepwd)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "rows = soup.findAll(\"div\", {\"class\":'item'})\n",
    "\n",
    "print len(rows)\n",
    "for row in rows: \n",
    "    urllinks.append(baseURL+row.find(\"div\", {\"class\": \"details-title\"}).find(\"a\")[\"href\"])\n",
    "    tempprice = re.match(r'\\$(\\d+),(\\d{3})', row.find(\"span\", {\"class\": \"price\"}).text)\n",
    "    prices.append(int(tempprice.group(1)+tempprice.group(2)))\n",
    "#     prices.append(row.find(\"span\", {\"class\": \"price\"}).text)\n",
    "    imagelinks.append(row.find(attrs = {\"class\": \"lazy\"})[\"data-original\"])\n",
    "    addresses.append(row.find(attrs = {\"class\": \"details-title\"}).find(\"a\").text.split(' #')[0])\n",
    "    details = row.findAll(attrs = {\"class\":  'details_info'})\n",
    "    sizeinfo = details[0].text\n",
    "    neighbtext = re.sub('\\s+', ' ', details[1].text)\n",
    "    neighb = re.match(r'.+Rental Unit in (.*)', neighbtext)\n",
    "    if neighb: \n",
    "        temp = neighb.groups(1)[0]        \n",
    "        neighborhoods.append(temp)\n",
    "    else:\n",
    "        neighborhoods.append('')\n",
    "\n",
    "    \n",
    "    snums= re.findall(r'(\\d+)', sizeinfo)\n",
    "    n = len(snums)\n",
    "    if n>=1:\n",
    "        beds.append(int(snums[0]))\n",
    "        if n>=2:\n",
    "            baths.append(int(snums[1]))\n",
    "            if n>2:\n",
    "                ft2s.append(int(snums[2]))\n",
    "            else: \n",
    "                ft2s.append(None)\n",
    "        else:\n",
    "            baths.append(None)\n",
    "    else: \n",
    "        beds.append(None)\n",
    "        \n",
    "    sourcepages.append(sourcedict[baseURL]) \n",
    "    hashids.append(calhashid(beds[-1], neighborhoods[-1], addresses[-1], prices[-1], sourcepages[-1]))\n",
    "        \n",
    "                \n",
    "URL= \"http://www.nybits.com/search/?_a%21process=y&_rid_=3&_ust_todo_=65733&_xid_=B8ek6K2lRUbcHi-1467834565&%21%21atypes=2br&%21%21rmin=1000&%21%21rmax=5000&%21%21fee=nofee&%21%21orderby=dateposted&submit=+SHOW+RENTAL+APARTMENTS+&%21%21nsearch=midtown_south&%21%21nsearch=sutton_place&%21%21nsearch=lincoln_square&%21%21nsearch=inwood\"\n",
    "baseURL = re.match(r'\\w+://(.+\\.\\w+)/', URL).group(0)\n",
    "filepwd=\"test3.htm\"\n",
    "html=codecs.open(filepwd)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "table = soup.find(\"table\", {\"id\": \"rentaltable\"})\n",
    "rows = table.findAll(\"tr\")\n",
    "\n",
    "print \"The number of rows in nybits: \", len(rows) \n",
    "\n",
    "\n",
    "tempaddresses = []\n",
    "tempimagelinks = []\n",
    "tempprices = []\n",
    "tempbeds = []\n",
    "tempneighborhood = []\n",
    "tempbedroom = []\n",
    "tempurllinks = []\n",
    "\n",
    "n = 3\n",
    "i=0\n",
    "def call_agent(url):\n",
    "    headers = { 'User-Agent' : 'Mozilla/5.0' }\n",
    "    req = urllib2.Request(url, None, headers)\n",
    "    return urllib2.urlopen(req).read()    \n",
    "\n",
    "    \n",
    "for row in rows:\n",
    "    if i>=n: \n",
    "        break\n",
    "    firtcolumn = row.find(\"td\", {\"class\": \"lst_sr_price\"})\n",
    "    if (not firtcolumn) or (len(firtcolumn.text)==0): \n",
    "        continue\n",
    "    secondcolumn = row.find(\"td\", {\"class\": \"lst_sr_topcell\"})\n",
    "    if not secondcolumn:\n",
    "        continue\n",
    "    featureimage = secondcolumn.find(\"img\")\n",
    "#     print featureimage\n",
    "    if featureimage: \n",
    "        i+=1\n",
    "        url = secondcolumn.find(\"a\")[\"href\"]\n",
    "        tempurllinks.append(url)\n",
    "        pricetext =  firtcolumn.text.replace('\\n', '')\n",
    "        print \"this is the first column: \", pricetext\n",
    "        pricestring = re.match(r'.*\\$(\\d+),(\\d{3})', pricetext) \n",
    "        tempprices.append(int(pricestring.group(1)+pricestring.group(2)))\n",
    "        html = call_agent(url)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        title = soup.find(\"div\", {\"id\": \"dancefloor\"}).find('h1').text\n",
    "        bedroom = re.match(r'(\\d).Bedroom', title).group(1)\n",
    "        tempbedroom.append(int(bedroom))\n",
    "\n",
    "\n",
    "\n",
    "        content = soup.find(\"div\", {\"class\": \"listing_content\"})\n",
    "        tempimagelinks.append(content.find(\"img\")[\"src\"])\n",
    "        table = soup.find(\"table\", {\"class\": \"listing_summary\"})\n",
    "        rows = table.findAll(\"tr\")\n",
    "        for row in rows: \n",
    "            columns = row.findAll(\"td\")\n",
    "            if \"Building\" in columns[0].text:\n",
    "                tempaddresses.append(columns[1].text.replace('\\n', ''))\n",
    "                continue\n",
    "            elif \"Neighborhood\" in columns[0].text:\n",
    "                tempneighb = columns[1].text.replace('\\n', '')\n",
    "                neigb = tempneighb.split(';')[-1]\n",
    "                tempneighborhood.append(neigb)\n",
    "        \n",
    "        hashids.append(calhashid(tempbedroom[-1], tempneighborhood[-1], tempaddresses[-1], tempprices[-1], 3))\n",
    "                \n",
    "        \n",
    "\n",
    "print tempaddresses, tempneighborhood, tempbedroom, tempprices\n",
    "\n",
    "prices += tempprices\n",
    "addresses += tempaddresses\n",
    "neighborhoods += tempneighborhood\n",
    "imagelinks += tempimagelinks\n",
    "urllinks += tempurllinks\n",
    "beds += tempbedroom\n",
    "\n",
    "for i in range(len(tempprices)):\n",
    "    baths.append(None)\n",
    "    ft2s.append(None)\n",
    "    sourcepages.append(3)\n",
    "    \n",
    "\n",
    "\n",
    "print len(addresses), len(beds), len(prices), len(baths), len(imagelinks), len(neighborhoods), len(urllinks), len(ft2s), len(sourcepages)\n",
    "finaldict = {'hashid': hashids,'full_address': addresses, 'bedroom':beds, 'price': prices, 'bathroom': baths, 'image_url': imagelinks, 'neighborhood': neighborhoods, 'linkurl': urllinks, 'source': sourcepages}\n",
    "                  \n",
    "df = pd.DataFrame(finaldict)\n",
    "print df                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c7c894a45ae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFirefox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetURLinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aehsanfar/anaconda/lib/python2.7/site-packages/selenium/webdriver/firefox/webdriver.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, firefox_profile, firefox_binary, timeout, capabilities, proxy, executable_path, firefox_options)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             executor = ExtensionConnection(\"127.0.0.1\", self.profile,\n\u001b[0;32m---> 80\u001b[0;31m                                            self.binary, timeout)\n\u001b[0m\u001b[1;32m     81\u001b[0m             RemoteWebDriver.__init__(\n\u001b[1;32m     82\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aehsanfar/anaconda/lib/python2.7/site-packages/selenium/webdriver/firefox/extension_connection.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, host, firefox_profile, firefox_binary, timeout)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch_browser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0m_URL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://%s:%d/hub\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mHOST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPORT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         RemoteConnection.__init__(\n",
      "\u001b[0;32m/Users/aehsanfar/anaconda/lib/python2.7/site-packages/selenium/webdriver/firefox/firefox_binary.pyc\u001b[0m in \u001b[0;36mlaunch_browser\u001b[0;34m(self, profile, timeout)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_from_profile_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_until_connectable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aehsanfar/anaconda/lib/python2.7/site-packages/selenium/webdriver/firefox/firefox_binary.pyc\u001b[0m in \u001b[0;36m_start_from_profile_path\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     88\u001b[0m         self.process = Popen(\n\u001b[1;32m     89\u001b[0m             \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             env=self._firefox_env)\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait_until_connectable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aehsanfar/anaconda/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags)\u001b[0m\n\u001b[1;32m    708\u001b[0m                                 \u001b[0mp2cread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2cwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m                                 errread, errwrite)\n\u001b[0m\u001b[1;32m    711\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0;31m# Preserve original exception in case os.close raises.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aehsanfar/anaconda/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite)\u001b[0m\n\u001b[1;32m   1333\u001b[0m                         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m                 \u001b[0mchild_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "\n",
    "# filepwd=\"locality.htm\"\n",
    "# html=codecs.open(filepwd)\n",
    "# soup = BeautifulSoup(html, \"html.parser\")\n",
    "# print soup.prettify()\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "import re\n",
    "\n",
    "driver = webdriver.PhantomJS() # or add to your PATH\n",
    "driver.set_window_size(1024, 768) # optional\n",
    "driver.get('https://www.zumper.com/apartments-for-rent/new-york-ny/no-fee/1-beds/under-2900')\n",
    "driver.save_screenshot('screen.png') # save a screenshot to disk\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# print len(addresses), len(beds), len(prices), len(baths), len(imagelinks), len(neighborhoods), len(urllinks)\n",
    "# finaldict = {'full_address': addresses, 'bedroom':beds, 'price': prices, 'bathroom': baths, 'image_url': imagelinks, 'neighborhood': neighborhoods, 'linkurl': urllinks}\n",
    "                  \n",
    "# df = pd.DataFrame(finaldict)\n",
    "# print df                \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sleeping for 5.504372 seconds\n",
      "I am sleeping for 7.265077 seconds\n",
      "I am sleeping for 7.389717 seconds\n",
      "The earlier html files than 2016-06-13 19:05 already exists!\n",
      "I am sleeping for 7.927196 seconds\n",
      "I am sleeping for 5.627328 seconds\n",
      "I am sleeping for 5.248326 seconds\n",
      "I am sleeping for 5.617581 seconds\n",
      "I am sleeping for 5.128154 seconds\n",
      "I am sleeping for 7.639993 seconds\n",
      "I am sleeping for 4.739344 seconds\n",
      "The earlier html files than 2016-06-13 16:39 already exists!\n"
     ]
    }
   ],
   "source": [
    "# nltk.download()\n",
    "\n",
    "def call_agent(url):\n",
    "    headers = { 'User-Agent' : 'Mozilla/5.0' }\n",
    "    req = urllib2.Request(url, None, headers)\n",
    "    return urllib2.urlopen(req).read()    \n",
    "\n",
    "def save_html(url, filename, latesthtmlfilename): \n",
    "    html = call_agent(url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    rows = soup.find(\"p\", {\"class\":\"row\"})\n",
    "    \n",
    "    if rows:\n",
    "        lasttime=rows.time['datetime']\n",
    "        newfilename='%s_%s.html'%(filename,lasttime)\n",
    "        if newfilename>latesthtmlfilename:\n",
    "            with open('Files/%s_%s.html'%(filename,lasttime), 'w') as fp: \n",
    "                fp.write(html)\n",
    "                fp.close()\n",
    "            return True\n",
    "        else: \n",
    "            print \"The earlier html files than %s already exists!\"%lasttime\n",
    "            return False\n",
    "    \n",
    "    else: \n",
    "        print \"No content for this page!!\"\n",
    "        return False\n",
    "    \n",
    "def save_pages(): \n",
    "    urls=[]\n",
    "    urls.append(\"http://newyork.craigslist.org/search/abo?sort=date&query=my%20apartment\")\n",
    "    urls.append(\"http://newyork.craigslist.org/search/roo?sort=date&query=my%20apartment\")\n",
    "    filenames=[\"abo_myapartment\",\"roo_myapartment\"]\n",
    "    \n",
    "    n=len(urls)\n",
    "    j=0\n",
    "    for i in range(n):\n",
    "        f=filenames[i]\n",
    "        htmlnames=[]\n",
    "        for file in os.listdir(\"Files\"):\n",
    "            if file.startswith(f[:3]) and file.endswith(\".html\"):\n",
    "                htmlnames.append(file)\n",
    "\n",
    "        htmlnames.sort()\n",
    "        latesthtmlfilename=htmlnames[-1]\n",
    "    \n",
    "        url=urls[i]\n",
    "        while j<1000:\n",
    "            sleeptime=4+4*np.random.random()\n",
    "            print \"I am sleeping for %f seconds\"%sleeptime\n",
    "            time.sleep(sleeptime)\n",
    "            url2=url+\"&s=\"+str(j)\n",
    "            filename=filenames[i]\n",
    "            didit=save_html(url2, filename, latesthtmlfilename)\n",
    "            if not didit:\n",
    "                break;\n",
    "            j+=100\n",
    "\n",
    "\n",
    "save_pages()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abo_myapartment_2016-06-14 15:43.html\n",
      "abo_myapartment_2016-06-15 13:36.html\n",
      "roo_myapartment_2016-06-13 22:39.html\n",
      "roo_myapartment_2016-06-14 07:16.html\n",
      "roo_myapartment_2016-06-14 12:32.html\n",
      "roo_myapartment_2016-06-14 14:23.html\n",
      "roo_myapartment_2016-06-14 17:56.html\n",
      "roo_myapartment_2016-06-14 23:23.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_bedrooms_from_title(title):\n",
    "    roomterms=[\"br\", \"rooms\", \"bedrooms\", \"bdrm\"]\n",
    "    title=title.lower()\n",
    "    if \"studio\" in title: \n",
    "        output=\"studio\"\n",
    "        return output\n",
    "    \n",
    "    temp=re.search('\\d+\\s?br', title)\n",
    "    if temp: \n",
    "        output=temp.group(0).replace(' ', '')\n",
    "        return output\n",
    "    else: \n",
    "        temp=re.search('\\d+\\s?bdrm', title)\n",
    "        if temp: \n",
    "            output=temp.group(0)\n",
    "        else: \n",
    "            title=title.lower()\n",
    "            temp=re.search('\\d+\\s?bedroom', title)\n",
    "            if temp: \n",
    "                output= temp.group(0)\n",
    "            else:\n",
    "                #print \"There is no bedroom in: \", title\n",
    "                output=None\n",
    "    \n",
    "    if \"loft\" in title: \n",
    "        if output:\n",
    "            nbeds=re.search('\\d+', output).group(0) \n",
    "            return nbeds+\"br loft\"\n",
    "        else: \n",
    "            return \"loft\"\n",
    "    else: \n",
    "        if output: \n",
    "            nbeds=re.search('\\d+', output).group(0) \n",
    "            return nbeds+\"br\"\n",
    "        else: \n",
    "            return None\n",
    "\n",
    "def find_apt_or_room(title):\n",
    "    title=title.lower()\n",
    "    terms=[\" room \", \" bedroom \", \"roommate\" ]\n",
    "    for t in terms: \n",
    "        if t in title: \n",
    "            return \"room\"\n",
    "        else: \n",
    "            return \"apt\"\n",
    "    \n",
    "def findlasttime(currenttime, ndays):\n",
    "    year=currenttime.year\n",
    "    month=currenttime.month\n",
    "    day=currenttime.day\n",
    "    \n",
    "    ndays=min(ndays,7)\n",
    "    if day>ndays:\n",
    "        return \"%s-%s-%s\"%(year, str(month).zfill(2), str(day-ndays).zfill(2))\n",
    "    else:\n",
    "        return \"%s-%s-%s\"%(year, str(month-1).zfill(2), str(day-ndays+30).zfill(2))\n",
    "\n",
    "\n",
    "def create_csv():\n",
    "    htmlnames=[]\n",
    "    csvnames=[]\n",
    "    for file in os.listdir(\"Files\"):\n",
    "        if file.endswith(\".html\"):\n",
    "            htmlnames.append(file)\n",
    "    #save_html(url, filename)\n",
    "    for file in os.listdir(\"Files\"):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csvnames.append(file[:-4])\n",
    "    \n",
    "    htmlnames=[h for h in htmlnames if h[:-4] not in csvnames]\n",
    "    \n",
    "    sectiondict={\"abo\": \"apt by owner\", \"roo\": \"rooms & shares\"}\n",
    "    for filename in htmlnames:\n",
    "        filepwd=\"Files/%s\"%filename\n",
    "        html=codecs.open(filepwd)\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        rows = soup.findAll(\"p\", {\"class\":\"row\"})\n",
    "        hreflinks=[]\n",
    "        titles=[]\n",
    "        prices=[]\n",
    "        datetimes=[]\n",
    "        neighborhoods=[]\n",
    "        beds=[]\n",
    "        aptorroom=[]\n",
    "        dublicate=[]\n",
    "        owner=[]\n",
    "        duplicate=[]\n",
    "        duplicatetime=[]\n",
    "        broker=[]\n",
    "        section=[]\n",
    "\n",
    "        for r in rows:\n",
    "            title=r.find(\"span\", {\"id\":\"titletextonly\"}).text\n",
    "            titles.append(title)\n",
    "            beds.append(find_bedrooms_from_title(title))\n",
    "            aptorroom.append(find_apt_or_room(title))\n",
    "            hreflinks.append(\"http://newyork.craigslist.org\"+r.a[\"href\"])\n",
    "            datetimes.append(r.time[\"datetime\"]) \n",
    "            reneighb = re.search(r\"\\(.*\\)\", r.find(\"span\", {\"class\":\"pnr\"}).text)\n",
    "            reprice=r.find(\"span\", {\"class\":\"price\"})\n",
    "            if reprice:\n",
    "                prices.append(int(reprice.text[1:]))\n",
    "            else: \n",
    "                prices.append(None)\n",
    "\n",
    "            if reneighb:\n",
    "                neighborhoods.append(reneighb.group(0)[1:-1])\n",
    "            else:\n",
    "                neighborhoods.append(None)\n",
    "            duplicate.append(0)\n",
    "            duplicatetime.append(0)\n",
    "            broker.append(0)\n",
    "            section.append(sectiondict[filename[:3]])\n",
    "\n",
    "\n",
    "\n",
    "        pagedict={\"title\":titles, \"datetime\": datetimes, \"type\": aptorroom, \"beds\": beds, \"price\":prices, \"neighborhood\": neighborhoods, \"broker\":broker,\"href\": hreflinks, \"duplicate\": duplicate, \"duplicatetime\": duplicatetime, \"section\": section}\n",
    "        df=pd.DataFrame(pagedict)\n",
    "        print filename\n",
    "        df.to_csv(\"Files/%s.csv\"%filename[:-4], sep=',', encoding='utf-8')    \n",
    "        \n",
    "\n",
    "create_csv()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6097 6097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#websites=['streeteasy':0 , 'nybits': 1, 'nakedapartments':3, 'hotpads':4, 'craigslist':5]\n",
    "\n",
    "\n",
    "def cal_simil(item1, item2):\n",
    "    nit1=len(item1)\n",
    "    minlength=min(len(item1), len(item2))\n",
    "    if minlength==0: \n",
    "        return 0\n",
    "\n",
    "    elif minlength==nit1: \n",
    "        shorter=item1\n",
    "        longer=item2\n",
    "    else: \n",
    "        shorter=item2\n",
    "        longer=item1\n",
    "\n",
    "    nwords=0\n",
    "    for word in shorter: \n",
    "        if word in longer: \n",
    "            nwords+=1\n",
    "    return float(nwords)/len(longer)\n",
    "                \n",
    "        \n",
    "def add_duplicatecolumn(df): \n",
    "    duplicate=[]\n",
    "    duplicatetime=[]\n",
    "    pricelist=list(df['price'])\n",
    "    titlelist=list(df['title'])\n",
    "    nonasciititles=[re.sub(r'[^\\x00-\\x7F]+',' ', text) for text in titlelist]\n",
    "    titlelist_token=[word_tokenize(t) for t in nonasciititles if len(t)>0]\n",
    "    neighborlist=list(df['neighborhood'])\n",
    "    neighborlist_token=[word_tokenize(str(n)) for n in neighborlist if len(str(n))>0]\n",
    "    datelist=list(df['datetime'])\n",
    "    N=len(pricelist)\n",
    "    for i in range(N): \n",
    "#         print i\n",
    "        neighb=neighborlist_token[i]\n",
    "        price=pricelist[i]\n",
    "        title=titlelist_token[i]\n",
    "        for j in range(i+1, N): \n",
    "            newprice=pricelist[j]\n",
    "            newtitle=titlelist_token[j]\n",
    "            newneighb=neighborlist_token[j]\n",
    "            d2=cal_simil(neighb, newneighb)\n",
    "            if d2>0.8:\n",
    "                d1=cal_simil(title, newtitle)\n",
    "                if newprice==price:\n",
    "                    d3=1\n",
    "                    similarity=sum([d1, d2, d3])/3.\n",
    "                else: \n",
    "                    similarity=sum([d1, d2])/2.\n",
    "                    \n",
    "                if similarity>0.75: \n",
    "                    duplicate.append(similarity)\n",
    "                    duplicatetime.append(datelist[j])\n",
    "                    break;\n",
    "                    \n",
    "            if j==N-1: \n",
    "                    duplicate.append(0)\n",
    "                    duplicatetime.append(None)\n",
    "                    \n",
    "        if i==N-1:\n",
    "            duplicate.append(0)\n",
    "            duplicatetime.append(None)\n",
    "\n",
    "\n",
    "    df=df.copy()\n",
    "    print df.shape[0], len(duplicate)\n",
    "    df['duplicate']=duplicate\n",
    "    df['duplicatetime']=duplicatetime\n",
    "    return df\n",
    "\n",
    "def add_broker(df):\n",
    "    duplicate=[]\n",
    "    duplicatetime=[]\n",
    "    titlelist=list(df['title'])\n",
    "    nonasciititles=[re.sub(r'[^\\x00-\\x7F]+',' ', text) for text in titlelist]\n",
    "    \n",
    "    alltitles=word_tokenize(' '.join(nonasciititles).lower())\n",
    "\n",
    "    brokerfactor=[]\n",
    "    titlerepetitionfactor=[]\n",
    "    titlekeywordfactor=[]\n",
    "    titlecapitalized=[]\n",
    "    allwords=[]\n",
    "    \n",
    "    allwords=word_tokenize(' '.join(nonasciititles))\n",
    "    wordcount=collections.Counter(allwords)\n",
    "    \n",
    "    for s in nonasciititles: \n",
    "        cl=sum([1 for c in s if c.isupper()])\n",
    "        sl=len(s)\n",
    "        titlecapitalized.append(float(cl)/sl)\n",
    "        words=word_tokenize(str(s))\n",
    "        titlerepetitionfactor.append(sum([wordcount[w] for w in words]))\n",
    "\n",
    "    maxcount=float(max(titlerepetitionfactor))\n",
    "    repetitionpercentile=[stats.percentileofscore(titlerepetitionfactor, a, 'strict') for a in titlerepetitionfactor]\n",
    "    brokerscore=[int((repetitionpercentile[i]+100*titlecapitalized[i])/2) for i in range(len(repetitionpercentile))]\n",
    "    filteredbrokerscore=[]\n",
    "    for x in brokerscore: \n",
    "        if x>=40:\n",
    "            filteredbrokerscore.append(x)\n",
    "        else: \n",
    "            filteredbrokerscore.append(0)\n",
    "            \n",
    "    df=df.copy()\n",
    "    df['broker']=filteredbrokerscore\n",
    "#     figure()\n",
    "#     hist(filteredbrokerscore)\n",
    "#     show()\n",
    "    return df\n",
    "\n",
    "    \n",
    "        \n",
    "def aggregate_csv(): \n",
    "    csvnames=[]\n",
    "    resultlistsofdf=[]\n",
    "    for file in os.listdir(\"Files\"):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csvnames.append(file)\n",
    "    csvnames.sort(reverse=True) \n",
    "    sections=[]\n",
    "    for name in csvnames: \n",
    "        sections.append(name[:3])\n",
    "    \n",
    "    sectionsset=list(set(sections))\n",
    "    sectionsset.remove('fin')\n",
    "    for sec in sectionsset: \n",
    "        tempnames=[name for name in csvnames if name.startswith(sec)]#.sort(reverse=True)\n",
    "        tempdfs=[]\n",
    "        previousdatetimelist=[]\n",
    "        for name in tempnames: \n",
    "            newdf=pd.read_csv(\"Files/\"+name)[[\"datetime\",\"type\", \"beds\", \"price\", \"neighborhood\", \"title\", \"href\", \"duplicate\", \"duplicatetime\", \"broker\", \"section\"]]\n",
    "            if len(previousdatetimelist)>0:\n",
    "                mindate=min(previousdatetimelist)\n",
    "#                 print name, mindate\n",
    "                newdf=newdf[newdf['datetime']<mindate]\n",
    "                if newdf.shape[0]==0:\n",
    "                    continue;\n",
    "\n",
    "            tempdfs.append(newdf)            \n",
    "            previousdatetimelist=newdf['datetime']\n",
    "#             print \"the new df shape is: \" , newdf.shape[0]\n",
    "\n",
    "        tempdf=pd.concat(tempdfs).sort_values(by=[\"datetime\"], ascending=0)\n",
    "#         print \"the tempdf size is:\", tempdf.shape[0]\n",
    "        resultlistsofdf.append(tempdf)\n",
    "        \n",
    "#     print \"the result list of df length is: \", len(resultlistsofdf)\n",
    "    df=pd.concat(resultlistsofdf).sort_values(by=[\"datetime\"], ascending=0)\n",
    "#     print \"The final df size is: \", df.shape[0]\n",
    "    n=df.shape[0]\n",
    "    df=df.set_index([range(n)])\n",
    "    df=add_broker(df)\n",
    "    df=add_duplicatecolumn(df)\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    \n",
    "#     with open(\"Files/finaltable.html\", 'w') as fp:\n",
    "#         fp.write(df.to_html().encode('utf8'))\n",
    "#         fp.close()\n",
    "    df[:200].reset_index().to_json(\"Files/finalCLListings.json\", orient='records'); \n",
    "    N=df.shape[0]\n",
    "    df[:min(5000, N)].to_csv(\"Files/finalcsv.csv\", sep=',', encoding='utf-8')    \n",
    "            \n",
    "    \n",
    "def search_pages():\n",
    "    filenames=[\"abo_myapartment\",\"roo_myapartment\"]\n",
    "    craigsection=[\"apt by owner\", \"rooms and shares \"]\n",
    "    n=len(urls)\n",
    "    csvnames=[]\n",
    "    for file in os.listdir(\"Files\"):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csvnames.append(file)\n",
    "    \n",
    "    csvnames.sort()\n",
    "    resultlistsofdf=[]\n",
    "#     if csvnames:\n",
    "#         latestdf=pd.read_csv(\"Files/\"+csvnames[-1])[[\"datetime\",\"type\", \"beds\", \"price\", \"neighborhood\", \"title\", \"href\", \"duplicate\", \"duplicatetime\", \"broker\"]]\n",
    "#         timelist=list(latestdf['datetime'])\n",
    "#         timelist.sort()\n",
    "#         lasttime=timelist[-1]\n",
    "#         resultlistsofdf.append(latestdf)\n",
    "    \n",
    "#     else: \n",
    "#         currenttime=datetime.datetime.now()\n",
    "#         lasttime=findlasttime(currenttime, 7)\n",
    "#     for i in range(n):\n",
    "#         dfs=[]\n",
    "#         url=urls[i]\n",
    "#         f=filenames[i]\n",
    "#         j=0\n",
    "        \n",
    "#         print lasttime\n",
    "#         resultlistsofdf.append(result[[\"datetime\",\"type\", \"beds\", \"price\", \"neighborhood\", \"title\", \"href\", \"duplicate\", \"duplicatetime\", \"broker\"]])\n",
    "                \n",
    "#             j+=100\n",
    "    \n",
    "#     print len(resultlistsofdf)\n",
    "\n",
    "#     df=pd.concat(resultlistsofdf).sort([\"datetime\"], ascending=0)\n",
    "#     timelist=list(df[\"datetime\"])\n",
    "#     timelist.sort()\n",
    "#     lasttime=timelist[-1]\n",
    "#     fn='Files/listings_%s.csv'%lasttime.replace(' ','_').replace(\":\",\"-\")\n",
    "#     df.to_csv(fn, sep=',', encoding='utf-8')    \n",
    "        \n",
    "        \n",
    "# finalresult=finalresult.sort([\"datetime\"], ascending=0)\n",
    "# finalresult.to_csv('Files/AptList.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "# previousresults=pd.read_csv('Files/previous_list.csv')\n",
    "\n",
    "#search_pages()\n",
    "aggregate_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
